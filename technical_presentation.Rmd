---
title: "Technical Project"
author: "Sarah Wigodsky, Dan Wigodsky, and Brian Weinfeld"
date: "5/7/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(tidyverse)
library(caret)
library(xgboost)
library(kernlab)
library(randomForest)
library(kableExtra)
library(glmnet)
library(DT)
library(corrplot)
library(car)

sysfonts::font_add_google(name = "Corben", family = "corben", regular.wt = 400, bold.wt = 700)
```

# {.tabset}

New regulations require us to better understand the manufacturing process of our beverages.  In this report, we will demonstrate the process by which we created a predictive model that will determine the PH level in a can. We will outline how we prepared the data and selected our predictive model. Finally, we will use the selected model to make predictions on a new set of data.

## Data Preparation

To prepare the data for building models, we plotted the missing data. There is very little missing data, and it is widely spread across the samples. This indicates that we should be able to successfully impute the missing data without worrying about injecting too much new information into the data set.

```{r warning=FALSE}
student.data <- readxl::read_excel('./data/StudentData.xlsx')
student.evaluation <- readxl::read_excel('./data/StudentEvaluation.xlsx')

missing.data.plot <- function(data){
  data %>%
    VIM::aggr(col=c('navyblue', 'yellow'),
      numbers=TRUE, sortVars=TRUE,
      labels=names(data), cex.axis=.6,
      gap=3, ylab=c('Missing Data', 'Pattern'), combined=TRUE
    )
}

missing.data.plot(student.data)
```

We discovered that four samples are missing the response variable PH. There are a number of ways to address this issue but ultimately we decided to simply remove these samples. The goal of the model is to predict the PH level and we were concerned about biasing the data by imputing these values. In addition there is one major outlier whose PH level is several orders of magnitude higher than the other values. Although we ultimately selected a modeling technique that is robust to outliers, we believed that this value was either incorrectly recorded or the result of a massive aberration that is unlikely to be seen again. Thus, we felt it best to remove this sample.

```{r}
student.data %>%
  ggplot(aes(PH, fill=PH > 9)) + 
  geom_histogram(bins=30) +
  theme_bw() +
  theme(legend.position='none') +
  labs(y='Count',
       title='PH Levels in Training Data')

student.data <- student.data %>%
  filter(!is.na(student.data$PH),
         student.data$PH < 9)
```

We created a simple data processing pipeline to fix a number of formatting issues with the data. We also one hot encoded the only categorical variable 'Brand'. Most importantly, this pipeline also imputes the missing data. We selected a powerful imputation method called MICE. In short, MICE functions by calculating a unique imputation model for each predictor treating that predictor as a response variable. This process is repeated several times until the imputed values stabilize.

```{r}
clean.cols <- function(name){
  name <- gsub('\\s', '', name)
  name <- tolower(name)
  return(name)
}

pipeline <- function(data){
  ph <- data$PH
  
  data <- data %>%
    select(-PH) %>%
    mutate(`Brand Code` = as.factor(`Brand Code`)) %>%
    rename_all(list(f = ~clean.cols(.))) %>%
    mice::mice(m=5, maxit=10, seed=123, printFlag=FALSE) %>%
    mice::complete(1)
  
    x <- predict(dummyVars("~ .", data=data), newdata=data) %>%
        as_tibble()
  
  return(cbind(ph, x))
}

student.data.complete <- pipeline(student.data)
student.evaluation.complete <- pipeline(student.evaluation)
```

Finally, we confirm that all the data has been properly imputed.

```{r}
missing.data.plot(student.data.complete)
```

```{r}
missing.data.plot(student.evaluation.complete)
```

With the data properly cleaned, we can now perform exploratory data analysis.

## EDA {.tabset}

After cleaning the data we proceeded to perform exploratory data analysis in order to better understand the data set that we are working with. Our data set consists of 36 variables. One is our target variable, ph. 4 Variables are based on brand. For regression models, we'll use three of these, with brand a as the base class. Based on Cook's distance, we removed 8 variables.  For our regression based models, this improved RMSE by $\approx .003$. 2354 had the highest Cook's Distance, $\approx .1$.

```{r fig.width=7, fig.height=7, warning=FALSE}
multi.model <- student.data.complete %>%
  select(-2) %>%
  filter(!row_number() %in% c(2354,2082,690,1896,1841,475,2562,2149)) %>%
  lm(data=., ph~.)

multi.model %>%
  influencePlot(id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
```

Within our model, 13 variables were significant at .001 significance: brand code b, mnfflow, carbpressure1, hydpressure3, temperature, usagecont, density, balling, pressurevacuum, oxygenfiller, bowlsetpoint, pressuresetpoint, ballinglvl.  An additional 6 variables were significant at .05.

```{r}
multi.model %>%
  summary()
```

We found strong levels of correlation (both positive and negative) for a variety of features. We can see stripes of dark color along balling and ballinglvl.

```{r fig.width=7,fig.height=7,fig.showtext = TRUE}
student.data.complete %>%
  cor(use='complete.obs') %>%
  corrplot(method='color', type='upper')
```

Some of our variables showed high correlation with each other. We can see stripes of dark color along balling and ballinglvl.  We check the variables for multi-collinearity.  Many of our variables were high in vif.  To remove this concern in our regression models, we built a model that removed variables until all had a vif score of less than 10.  

```{r fig.width=13, fig.height=5, fig.showtext=TRUE, warning=FALSE}
student.data.complete %>%
  select(c(1:2, 4:36)) %>%
  lm(data=., ph~.) %>%
  vif() %>%
  tibble('variable' = names(.), 'variable_vif' = .) %>%
  filter(variable_vif > 10) %>%
  ggplot(aes(variable, variable_vif)) + 
  geom_bar(stat='identity', fill='#b5c6fc') + 
  theme(panel.background = element_rect(fill = '#707996'),
        text = element_text(family = 'corben', color='#249382', size=35),
        axis.text.x = element_text(angle = 30, hjust = .9)
        ) + 
  labs(x = 'Variable', 
       y = 'VIF',
       title = 'Variables with the highest vif')
```

Below is a comparison of the distributions between each feature and the response variable PH.

```{r fig.height=18, fig.width=10}
student.data.complete %>%
  gather(key='key', value='value', -ph) %>%
  ggplot(aes(value, ph)) + 
  geom_point(alpha=.9,color='#65b285') + 
  facet_wrap(~key, scales='free_x', ncol=4) +
  labs(y='PH')
```

## Model Development {.tabset}

We began our exploration for a predictive model by separating out 20% of the training data to serve as our final validation data. We also split each data frame into a list for easier use in testing various predictive models.

```{r}
set.seed(123)

listify <- function(data){
  return(list('y' = data$ph, 'x' = data %>% select(-ph)))
}

part <- createDataPartition(student.data.complete$ph, p=0.8, list=FALSE)
training <- student.data.complete %>%
  filter(row_number() %in% part)
validation <- student.data.complete %>%
  filter(!row_number() %in% part)

student.data.complete <- listify(student.data.complete)
student.evaluation.complete <- listify(student.evaluation.complete)
training <- listify(training)
validation <- listify(validation)
```

We iterated over numerous different predictive models including a variety of different regression based and rule based models. The code for all of these tests is available in our repo. We ultimately settled on using a Random Forest model due to it achieving the best performance in our cross fold validated testing. We assessed the performance of each model based on the root mean square error.  The tabs below show a selected set of the models we trained.

This model achieved an RMSE of $\approx0.0993$ in our cross validated testing and an $R^2 \approx 0.66$.

### XG Boost

```{r}
grid <- expand.grid(nrounds = 2500,
                    max_depth = 6,
                    eta = 0.03,
                    gamma = 0,
                    colsample_bytree = 1,
                    min_child_weight = 1,
                    subsample = 0.5)

control <- trainControl(method='cv',
                        number=10,
                        allowParallel = TRUE)

model <- train(x = training$x,
             y = training$y,
             method = 'xgbTree',
             tuneGrid = grid,
             metric = 'RMSE',
             trControl = control)

model$results
```

The best XG Boost model performed nearly as well as the Random Forest model and would serve as a good alternative choice for a model, if needed. 

### Support Vector Machine

```{r, message=FALSE, warning=FALSE,cache=TRUE}
set.seed(200)
svmRTuned <- train(x = training$x,
                   y = training$y,
                   method="svmRadial", 
                   preProc=c("center","scale"),
                   tuneLength = 14,
                   trControl = trainControl(method="cv"))

svmRTuned
svmRTuned$finalModel
```

### Elastic Net

```{r}
training_set_matrix<-as.matrix(training$x)

elastic_net_model<-glmnet(as.matrix(training_set_matrix[,-1]), training$y, family="gaussian", alpha=.65, standardize = TRUE)

elnet_predict<-predict(elastic_net_model, s=elastic_net_model$lambda.1se, newx=as.matrix(training_set_matrix[,-1]))
RMSE(elnet_predict, training$y)
```

### Random Forest

```{r}
set.seed(200)
rf <- randomForest(x = training$x,
                   y = training$y, 
                   importance=TRUE,
                   ntree=1000)
mean(sqrt(rf$mse))
mean(rf$rsq)
```

Below we can see the order of importance for each of the predictors in determining the response variable. This table also indicates that there are a number of predictors that could possibly be dropped from the model with little loss to the model's predictive value. This may be an interesting avenue of exploration if we wanted to prioritize a model that can be easily interpreted. As it stands, while this model is highly accurate, it is not easy to interpret.

```{r}
rf$importance %>%
  as.data.frame() %>%
  rownames_to_column(var='predictor') %>%
  arrange(desc(`%IncMSE`)) %>%
  datatable()
```

With the model selected, we will finally run the model against the withheld validation data. This will give us a sense of how well the model will perform on data that it has never seen before. Our model scores a strong RMSE of approximately 0.0975 with an $R^2$ of 0.7.  

```{r}
postResample(predict(rf, newdata=validation$x), validation$y)
```

With the model trained and validated, we can make our final predictions

## Predictions

Before making our final predictions we are going to retrain the Random Forest model one last time. The model will be retrained with the same hyper-parameters, however this time we will train on ALL of the training data, including the previously withheld validation data. This step is often considered optional when creating a predictive model. Folding the validation data back into the model, while leaving everything else the same, can often allow our model to eek out just a bit more predictive ability due to the extra samples. However, the improvement is often very small (if at all) and as such this step will often be ignored if the training process is long. In our case, as the data set is small, we believe there is no downside to retraining the model.

```{r}
final.rf <- randomForest(x = student.data.complete$x,
                   y = student.data.complete$y, 
                   importance=TRUE,
                   ntree=1000)
```

With the final model trained, we can finally make our predictions on the provided data.

```{r}
predictions <- predict(final.rf, newdata=student.evaluation.complete$x)
predictions %>%
  tibble::enframe(name = NULL) %>%
  datatable()
```

Finally, as requested, we can write out the file in an Excel readable format.

```{r}
predictions %>%
  tibble::enframe(name = NULL) %>%
  rownames_to_column() %>%
  rename(PH = value, 
         row = rowname) %>%
  write_excel_csv('./data/predictions.csv')
```
